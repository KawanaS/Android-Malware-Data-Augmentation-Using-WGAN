# -*- coding: utf-8 -*-
"""
Created on Fri May 27 18:49:50 2022

@author: hp
"""

import numpy as np
import matplotlib.pyplot as plt
import cv2
import os
import random
import pickle
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Conv1D, MaxPooling2D, MaxPooling1D, Flatten, Dense, Dropout, Activation
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

config = tf.compat.v1.ConfigProto(device_count={'GPU': 1})
config.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=config)

DATA_DIR = 'E:/Python/Machine_Learning/Android_source_code_analysis/classes_dex+xml'
CATEGORIES = ['goodware', 'malware']
IMG_SIZE = 32

for category in CATEGORIES:
    # path to goodware and malware images used for training
    path = os.path.join(DATA_DIR, category)
    class_num = CATEGORIES.index(category)
    for img in os.listdir(path)[:4]:
        img_array = cv2.imread(os.path.join(path,img))
        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
        plt.imshow(new_array)
        plt.title(img)
        plt.legend(title=f"class is {class_num}.")
        plt.axis("off")
        plt.show()
              
'''print(img_array.shape)
new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))
plt.imshow(new_array)
plt.show()
print(new_array.shape)'''

training_data = []

def create_training_data():
    for category in CATEGORIES:
        # path to goodware and malware images used for training
        path = os.path.join(DATA_DIR, category)
        class_num = CATEGORIES.index(category)
        for img in os.listdir(path):
            img_array = cv2.imread(os.path.join(path, img),cv2.IMREAD_GRAYSCALE)
            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
            training_data.append([new_array, class_num])

create_training_data()
print(len(training_data))
# shuffle the data to prevent the model from generalizing
random.shuffle(training_data)

for sample in training_data[:4]:
    print(sample[1])

X = []
y = []

for features, label in training_data:
    X.append(features)
    y.append(label)

X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)
y = np.array(y)
print(np.unique(y))

# scale the features
#scaler = MinMaxScaler(feature_range=(0,1))
#X = scaler.fit_transform(X)

# create pickle file for X data
'''pickle_out = open('X.pickle','wb')
pickle.dump(X,pickle_out)
pickle_out.close()

#create pickle file for y data
pickle_out = open('y.pickle','wb')
pickle.dump(y,pickle_out)
pickle_out.close()

pickle_in = open('X.pickle','rb')
# load X data from pickle
X  pickle.load(pickle_in)
print(X[1])'''

#X = pickle.load(open('X.pickle','rb'))
#y = pickle.load(open('y.pickle','rb'))

X = X/255.0
#X = X.reshape(X.shape[1:])
#X = X.transpose()
print(X.shape)
print(y.shape)

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=12345)

opt = tf.keras.optimizers.Adam(learning_rate=0.001)

model = Sequential()
model.add(Conv2D(64, (3, 3), input_shape=(X.shape[1:])))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(64, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=opt,
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['acc',
                        tf.keras.metrics.Precision(),
                        tf.keras.metrics.Recall(),
                        tfa.metrics.F1Score(num_classes=2, average="micro", threshold=0.5)])

history=model.fit(x_train, y_train, batch_size=10,epochs=19,validation_split=0.1)
#history = model.fit(X, y, batch_size=10, epochs=119, validation_split=0.1)

'''loss_train = history.history['loss']
loss_val = history.history['val_loss']
epochs = range(1, 120)
fig, ax = plt.subplots()
ax.scatter([0.25], [0.25])
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


loss_train = history.history['acc']
loss_val = history.history['val_acc']
epochs = range(1, 120)
fig, ax = plt.subplots()
ax.scatter([1], [1])
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='Validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()'''

model.save('model_3-dex+xml.h5')
